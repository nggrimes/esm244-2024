---
title: "NLS Student work up"
author: "Nathan Grimes, Casey O'Hara"
format: 
  html:
    code-fold: true
    embed-resources: true
execute: 
  echo: true
  message: false
  warning: false
editor: visual
---

## Introduction

To demonstrate the power of non linear least squares in R we're going to recreate a fisheries paper that examined whether productivity in fisheries was driven more by abundance, regime shifts, or simply random noise. [Here is a link to the paper if curious about detailied methods and results](https://www.pnas.org/content/110/5/1779). In their research, they used maximum likelihood estimation rather than non-linear least squares, but we will see similar results. In fact, the model choices for nls and mle are nearly identical in selected coefficients! Also to simplify the lab, we will only recreate the abundance and random models.

### Data

All data comes from the [RAM Legacy Database](https://www.re3data.org/repository/r3d100012095), normally we would have to go through the whole database, but instead I have extracted out the main table containing all the values of stock parameters and a list of stocks within the cod and sole families to examine. Anyone interested in fisheries should familiarize themselves with RAM because it is the best set of fisheries data across different stocks, countries, and time. 

Load in the data.

```{r}
load(here::here("data","lab_6","fish_data.Rdata"))

library(tidyverse)
library(Metrics)
library(cowplot)
library(here)
```


## Single model NLS

### Step 1:

Surplus is the excess amount of biomass that was added or taken from the underlying stock. It can be modeled as a simple addition. Surplus also allows us to model recruitment, growth, and natural mortality that is often difficult data to collect. Stock assessments, that RAM is built on, allows us to easily back out surplus using historical catch and biomass records.

\begin{equation}
S_t=B_{t+1}-B_t+C_t
\end{equation}

We will need to add a column in our dataset calculating surplus in any given year. Since we have a variable from the future we can use the `lead()` function. Make sure to drop the `NA` created by the lead function. 

Break down the following code chunk by adding comments at each line describing what is happening at each step and why need to do it.

```{r}
surplus<-Fish_data |> 
  group_by(stockid) |> 
  select(stockid,year,TBbest,TCbest) |> 
  drop_na() |> 
  mutate(f_biomass=lead(TBbest)) |> 
  mutate(surplus=f_biomass-TBbest+TCbest) |> 
  drop_na()
  
  
```


Let's see what our data looks like with an example of one stock.

```{r}
one_stock<-surplus |> 
  filter(stockid=="COD1f-XIV")

ggplot(data=one_stock,aes(x=year,y=surplus))+
  geom_point(size=3,color="black")+
  theme_minimal()

```


### Step One: Selecting a Model

Non-linear least squares needs a model to fit the data. Understanding which model to use relies on your expertise and ability to justify its selection. Since we're trying to predict fisheries surplus, we should examine the surplus-production literature. 

There are three primary surplus-production models in the fishery world. The most common is the Gordon-Schaefer model. Vert-pre et al., use a Fox-Model that typically provides a more conservative estimate of maximum sustainable yield. The last model is the Pella-Tomslison model that really is just a more flexible model of the other two using a shape parameter $\phi$ to control the curve. All are built on a logistic growth curve. Given a level of biomass we will be able to predict what the surplus ought to be if we know (or will determine) the maximum sustainable yield and the carrying capacity. Maximum sustainable yield simply refers to the amount of biomass that facilitates the greatest level of harvest possible without depleting the stock. Carrying capacity is the upper bound on the total population size and represents natural environmental pressure limiting stock growth. The paper uses a simplified Fox model that we try to find parameters for to fit the fishery data.  

$$
\hat{S_t}=-e*MSY(\frac{B_t}{K})\ln(\frac{B_t}{K})
$$

Where e is base of the natural log $\approx$ 2.718, MSY is the maximum sustainable yield, K is the carrying capacity, and $B_t$ is the biomass for the observed year.

Let's create a function in R.

```{r foxmodel}
fox<-function(m,carry,biomass){
 out= -2.718*m*(biomass/carry)*log(biomass/carry)
return(out)
}


```


It is always a good idea to test your function to make sure it works and to visualize how our model will fit the data.

Arrange the following out of order code to make a plot that adds an example Fox model onto our first graph.

```{r}
#| eval: false


labs(x="",y="Surplus Production")+
  
geom_line(aes(x=year,y=predict,color="Model"),linewidth=2)+
  
carry=150000

scale_color_manual(values=c("Data"="black","Model"="red"))+
  
mutate(predict=fox(m,carry,TBbest))

ggplot(data=fox_sim,aes(x=year,y=surplus,color="Data"))+
  
geom_point(size=3)+

fox_sim<-one_stock |>   

theme_minimal()

m=60000
```


### Step Three: Initial Guess

Now we can construct our nonlinear least squares with sufficient guesses. But what should our guesses be? Well carrying capacity is straightforward. Traditionally, its estimated as the highest observed biomass so we can just take the max of the biomass data. Maximum sustainable yield can be found analytically. It's been done many times so I'll just tell you it's estimated at 37% of the carrying capacity.

```{r guess}

#Write out the guess first, we'll move into the nls wrapper soon

#Since MSY comes into the fox function first, it needs to be the first guess

guess_vec=c(max(one_stock$TBbest)*0.37,
            max(one_stock$TBbest))

```

### Step Four: Run NLS

Fill in the nls function to make our first model work!

```{r nlsonemodel}
#| eval: false
one_stock_nls=nls(formula=????,
                  data=?????,
                  start=??????,
                  trace=????)
```


### Step Five: Evaluate

Use the model to make predictions and plot the new predictions

```{r}
#| eval: false

#What commands can we use here
one_stock_predict<-one_stock |> 
  ?????

# Code to plot
ggplot(data=one_stock_predict)+
  geom_point(aes(x=year,y=surplus))+
  geom_path(aes(x=year,y=predict),color='red'))
```


Calculate the RMSE of the prediction

```{r}
## Either make your own rmse function or use the Metrics::rmse function
```



Great our model works on a single model! Now we need to find a way to replicate the analysis. Ideally without using for loops as those can be a pain to account for. 

## Using purrr to run many nls models

Write some pseudocode to define an approach to create apply purrr to all



### Set up framework

Run the following code chunk. Examine `fox_all` dataframe to help you build the nls function wrapper for purrr. What does nest() do and why is that useful to us?

```{r}
fox_all<-surplus |>
  group_by(stockid) |> 
  nest()
```

Create a wrapper function to apply an nls model to our data. How do we provide guessess?

```{r nlsmany}
#| eval: false  
#Define a new function to pass along the nls calls

all_nls_fcn<-function(surplus_df){ # Where will the surplus_df come from?
  nls(surplus~fox(m,carry,TBbest),
  data=surplus_df,
  start=????? # How are we going to define the guesses for each new species?
}
```


This code chunk runs a purrr mapping of build an nls, evaluating the nls, and getting our rmse scores in one step. However, it's broken. Run through each pipe to find the broken error. Fix the error and run the whole code chunk. There is one "error" that won't pop up, but our final output will be difficult to handle. Correct this mistake as well.

```{r}
#| eval: true

fox_all<-surplus |>
  group_by(stockid) |> 
  nest() |> 
  mutate(nls_model=map(data,~nls(.x))) |> 
  mutate(predictions=map2(nls_model,data,~predict(.x,newdata=.y))) |> 
  mutate(RMSE=map2_dbl(predictions,data,~Metrics::rmse(.x,.y$surplus)))
```


## Compare to a random null model

In the paper, they derive a null model to test the different models against. The best way to test if any of these models are better is if they can out perform a random collection of data. They propose if our models can't outpeform the average surplus in the time frame, then the stock is under more influence of sheer randomness then any explicable measures. We can jump straight into the purrr analysis. 

```{r}

# Define the random null model as the rmse
r_avg<-function(surplus){
  avg_sur=mean(surplus)
  
  rmse=sqrt(mean((avg_sur-surplus)^2))
  
  return(rmse)
}


r_mse<-surplus |>
  group_by(stockid) |> 
  nest() |> 
  mutate(RMSE=map_dbl(data,~r_avg(.x$surplus)))
```

## How did the models compare to the null?

Look at the outputs and find a way to compare the RMSE for each model output. How did our fox model compare to the random null model?

```{r}
## Perform analysis in this code chunk.
```

In the paper, about 12% of the stocks were more explained by random shocks and 16% more so by abundance models. The rest were led by regime shifts that we did not model. Our results only found one stock out of 44 was better explained by random growth. Either our choice of nls models is far better than their mle method (not likely), or the subset I chose lends itself more to abundance models (this is really what happened). I did not want to overwhelm the analysis with over 200 stocks. Cod species in their paper typically were best explained by abundance models. Since our dataset focuses on cod, it's unsurprising that the Fox model best predicts cod surplus.

## Graph the top 5 best fit models

Purrr combined with cowplot creates a streamlined way to build multiple graphs. Let's take the 5 best fit Fox models and show how they performed compared to the data. I will walk through this code.

```{r}
plots<-fox_all |> 
  arrange(RMSE) |> 
  head(5) |> 
  mutate(graph=map2(data,predictions,
                    ~ggplot()+
                      geom_point(data = .x,aes(x=.x$year,y=.x$surplus,color='Actual'))+
                      geom_point(aes(x=.x$year,y=.y,color='Predicted'))+
                      theme_minimal()+xlab('')+ylab('Surplus')+
                      scale_color_manual(name="Legend",breaks = c('Actual','Predicted'),values=c('Actual'='black','Predicted'='red'))))

#Make a new list
plot_list=plots$graph

cowplot::plot_grid(plotlist=plot_list,labels =c( plots$stockid,""),hjust=-0.5,vjust=1)


```

### Extra Cowplot fun to make it look really clean

```{r legends}
#extract the legend

legend<-get_legend(plots$graph[[1]])

#remove the legend from the plots
# sometimes you don't always need to use purrr, a quick for loop for something easy can also work
for(i in 1:length(plots$graph)){
  plots$graph[[i]]<-plots$graph[[i]]+theme(legend.position = "none")
}

#Make a new list
plot_list_legend=plots$graph

#create a new plot in the empty space of cowplot grids and fill it with the legend info we took from the plots.

plot_list_legend[[6]]<-legend

cowplot::plot_grid(plotlist=plot_list_legend,labels =c( plots$stockid,""),hjust=-0.5,vjust=1)
```

