---
title: "Non-linear Least Squares"
subtitle: "ESM 244 2024"
author: "Nathaniel Grimes"
institute: "Bren School of Environmental Science"
date: last-modified
format: 
  revealjs:
    chalkboard: true
    slide-number: true
    show-slide-number: print
    theme: [default, ucsb.scss]
editor: visual
---

------------------------------------------------------------------------

## Presentation title {.textcenter}

![](img/ucsb_logo.png){.absolute top="9" left="0" width="400" height="20"}

### Presentation subtitle

<br/>

## \### Authorship name {.gold}

------------------------------------------------------------------------

## Quick Aside

I made this presentation in Quarto using RevealJS. The code is up on Canvas if you want to see more ways Quarto can be used beyond just homework assignments

::: columns
::: {.column width="55%"}
**Pros**

-   Easily integrate R code

-   Update figures automatically

-   Living presentation with html features

-   Easier to write math through Latex and MathJax
:::

::: {.column width="40%"}
**Cons**

-   Define everything in code, no easy powerpoint tools

-   Maybe not as "sexy" as other presentations
:::
:::

## Packages to follow along with

```{r}
#| echo: true

library(tidyverse)
remotes::install_github("lter/lterdatasampler")
library(lterdatasampler)
library(knitr)
library(broom)
library(investr)
library(kableExtra)
```

# What is Non-linear Least Squares? {background-color="#003660"}

## Remember what Ordinary Least Squares does first

The Fundamental Objective of OLS

::: columns
::: {.column width="45%"}
```{r, fig.dim=c(6,6)}
fit<-lm(mpg~hp,data=mtcars)

d<-mtcars
d$predicted<-predict(fit)
d$residuals<-residuals(fit)

ggplot(d,aes(x=hp,y=mpg))+
  geom_point(size=4)+
  theme_bw()+
  theme(axis.title = element_text(size=28),axis.text = element_text(size=20))
  
```
:::

::: {.column width="55%"}
::: fragment
-   Best Fit a Line to Data
:::

::: fragment
-   How Does OLS Fit a Line to Data?

Hint:

$\hat{\beta}=\frac{\sum^n_{i=1}(x_i-\bar{x})(y_i-\bar{y})}{\sum^n_{i=1}(x_i-\bar{x})^2}$
:::
:::
:::

## Remember what Ordinary Least Squares does first

<br/>

::: columns
::: {.column width="45%"}
```{r,fig.dim=c(6,6)}

#| fig-align='center'
#| fig-width=6
#| fig-height=10
ggplot(d,aes(x=hp,y=mpg))+
  geom_smooth(method = "lm",se=FALSE,color="blue")+geom_segment(aes(xend=hp,yend=predicted),alpha=.2)+
  geom_point(size=4)+
  geom_point(aes(y=predicted),size=4,shape=1)+
  theme_bw()+
  theme(axis.title = element_text(size=28),axis.text = element_text(size=20))
```
:::

::: {.column width="55%"}
How does OLS fit the line?

-   Minimize squared error (aka residuals)

$\hat\beta=\min_\beta \sum^n_{i=1}\hat{\epsilon_i}^2=\sum^n_{i=1}(y_i-\beta x_i)^2$
:::
:::

## OLS is simple and powerful, but has limitations

-   Linear relationship between predictor (y) and variables (x)

-   Last week we started branching away from linear models with logistic regressions

-   But the link functions typically still maintain a linear form

$$
\ln(\frac{p}{1-p})=\beta_0+\beta_1x_1+\beta_2x_2+...\beta_nx_n
$$

## But what if we get something like this?

```{r, fig.dim = c(12, 7),fig.align='center'}
knz_bison_age <- knz_bison %>% 
  mutate(animal_age = rec_year - animal_yob) %>% 
  filter(animal_sex=="F")
  

ggplot(data=knz_bison_age)+
  geom_point(aes(x=animal_age,y=animal_weight),size=2.5,alpha=0.2,color='purple')+
  theme_minimal()+
  xlab('Age')+
  ylab('Weight')+
  ggtitle("Female Bison from Konza Prairie")+
  theme(axis.title = element_text(size=26),axis.text = element_text(size=20))+
  theme(plot.title = element_text(size=28,hjust=0.5))
```


## Or this?

![](img/refractivity.PNG){width=90%,height=90%,fig-align="center"}

## In specific applications, accuracy greatly matters!

![](img/vtrpe_run.PNG){fig-align="center"}

## Nonlinear Least Squares

Apply the same idea of least squares error minimization, but with any function

$$
\begin{aligned}
y_i&=f(x_i,\boldsymbol\beta)+\epsilon_i &\text{(1)}\\
\min_{\boldsymbol\beta}&=\sum^n_{i=1}\epsilon_i^2=\sum^n_{i=1}(y_i-f(x_i,\boldsymbol\beta))^2 &\text{(2)}
\end{aligned}
$$


General idea is very similar, but implementation and use is quite different

::: {.notes}
Notice the xB is replaced by a general function.
:::

## How NLS works

No simple analytical solution like in OLS (Solve for $\hat\beta$)

Iteratively approximate the solution through algorithms

- Gauss-Newton (Most Common)

- Levenberg-Marquardt (More flexible)

Approximate the function's gradient (think derivative), then move along until a convergence criteria is met

$$
|\frac{\overbrace{S^k}^{\text{Previous squared errror}}-\overbrace{S^{k+1}}^{\text{Updated squared error}}}{S^k}|<0.0001
$$



## Demonstration of Gauss-Newton Algorithm {#sec-algo}

:::: {.columns}

::: {.column width="50%"}
![](https://i.stack.imgur.com/gdJ3v.gif)
:::

::: {.column width="50%"}

- Global minimum at a=2.25

- Begins at initial guess of a=3.5

- Step size depends on the 2nd order approximation

- Keeps going until the [green]{.seagreen} line (first derivative) reaches close to zero
:::

::::


## Why should we use NLS?

We need far fewer assumptions than multiple regression

- Residuals do not have to be normally distributed

- No linear relationship required

- Don't care about homoscedasticity

If underlying model is smooth, can find solutions accurately and quickly compared to other methods

## When to use NLS

Best suited for specific model parameterization given a collection of data

Have a known equation and want to fit parameters 

[There is no $R^2$ value to compare across model specifications, but we can still test model performance using AIC or Cross Fold Validation through RMSE (In lab this week!)]{.small-text}



## Pitfalls (literally) and warnings

:::: {.columns}

::: {.column width="50%"}

NLS is only as good as the underlying model. [Bring your brain to the party]{.gold-bold} and make sure the model you're fitting is appropriate

Follows gradient of steepest descent $\rightarrow$ local min/max valleys

- With n-parameters chances of local min/max rises
:::

::: {.column width="50%"}

```{r, out.width="85%",fig.align='center'}
knitr::include_graphics("img/minmax.png")
```

Requires good initial guesses

- Comes from algorithms [[Click back to slide](@sec-algo)]{.small-text}
:::

::::

## Know when to use NLS or another option

```{r, out.width="55%",fig.align='center'}
knitr::include_graphics("img/vtrpe_run.PNG")
```


- In this research we ended up using Genetic Algorithms instead as they provide global solutions without having to guess

- Operationally, the Navy doesn't have time to guess


# Using NLS in R {background-color="#003660"}

## Let's apply NLS to our Female Bisons

```{r bisonfollow,echo=TRUE}
knz_bison_age <- knz_bison %>% 
  mutate(animal_age = rec_year - animal_yob) %>% 
  filter(animal_sex=="F")
```


```{r bisongraph, out.width="35%",fig.align='center'}
knz_plt<-ggplot(data=knz_bison_age)+
  geom_point(aes(x=animal_age,y=animal_weight),size=1,alpha=0.2,color='purple')+
  theme_minimal()+
  xlab('Age')+
  ylab('Weight')+
  ggtitle("Female Bison from Konza Prairie")+
  theme(axis.title = element_text(size=28),axis.text = element_text(size=20))+
  theme(plot.title = element_text(size=28,hjust=0.5))

knz_plt
```

## Use R Built in functions


```{r nls, echo=TRUE,eval=FALSE}
df_nls<-nls(formula=   # Model we want to estimate,
            data   # Data we are evaluating,
            start  # Our initial guesses
            control # List of tolerance value, etc
            trace  # Do we want to see convergence
            upper  # Bounds on input parameters
            ... # some other useful stuff )
```

## What Model to use?

Scour the literature or create your own (only with sufficient justification)

Martin and Barboza (2020) used a Gompertz model

$$BM=b1*exp(-exp(-b2*(age-b3)))$$

:::: {.columns}

::: {.column width="50%"}

[$b1$ = asymptotic body mass (pounds)]{.small-text}

[$b2$ = instantaneous growth-rate]{.small-text}

[$b3$ = age at inflection point years]{.small-text}

[$age$ = Independent variable]{.small-text}

[$BM$ = Body mass (pounds) Dependent variable]{.small-text}
:::

::: {.column width="50%"}

```{r gompertz,out.width="85%"}
gompertz<-function(b1,b2,b3,age){
 BM= b1*exp(-exp(-b2*(age-b3)))
return(BM)
}

b1=800
b2=1.25
b3=0.5

x=seq(from=0, to=20,length.out=100)

g1=gompertz(b1,b2,b3,x)

df1=data.frame(age=x,weight=g1)

df1plt<-ggplot(data=df1,aes(x=age,y=weight))+
  geom_line(color="darkorchid",size=2)+
  theme_minimal()+
  ylim(c(50,1000))+
  ggtitle("Example Gompertz")+
  theme(axis.title = element_text(size=28),axis.text = element_text(size=20))+
  theme(plot.title = element_text(size=30,hjust=0.5))

df1plt
```
:::

::::

## Create a function in R to test our model

```{r model, echo=TRUE}
gompertz<-function(b1,b2,b3,age){
 BM= b1*exp(-exp(-b2*(age-b3)))
return(BM)
}
```


<br/>

<br/>

[Note: For the nls function it's okay to define all parameters like we did. In other optimization tools (e.g. optim) you would want to keep the first input index as a vector if you have multiple choice variables]{.small-text}

## Providing a guess is very important

```{r nlsguess, echo=TRUE,eval=FALSE}
#| code-line-numbers: "|"
df_nls<-nls(animal_weight~gompertz(b1,b2,b3,animal_age),   
            data=knz_bison_age,   
            start=list(b1=?,b2=?,b3=?),
            trace=TRUE )
```

<br/>

The initial guesses and `data` variable also tell nls which variables we are trying to find from our data

## 4 methods for providing guesses

1) Use past parameters from similar studies


2) Use data to internally define guesses (min, mean, max, etc.)

3) In 2-D, look at the graphs and estimate


4) In N-D, combine steps 1-2 then create a start grid to search over

## Applied guessing

:::: {.columns}
::: {.column width="50%"}
How we could use step 2 and 3 in this case?

Asymptotic body mass $b1$ implies a max body length we could take the biggest observed female or generally look at the graph

Age inflection point $b3$ is where the curve starts bending

Instantaneous growth-rate $b2$ is kind of weird, but you could manipulate a set Gompertz model to see how it changes the shape and try to match it.
:::

::: {.column width="50%"}

```{r combeinplot, out.height='50%',fig.align='center'}
knz_plt+ geom_line(data=df1,aes(x=age,y=weight),color="blue",size=2)
```
:::
::::

## Apply guesses

```{r nlsrun,echo=TRUE,results=TRUE}
b_gompertz<-nls(animal_weight~gompertz(b1,b2,b3,animal_age),
                      data = knz_bison_age,
                      start = list(b1=1000,b2=1,b3=0.6),
                      trace = TRUE)
```

## What did the model find?

```{css echo=FALSE}
.small-code{
font-size: 75%;
}
```

<div class=small-code>
```{r nlssummary,echo=TRUE,results=TRUE}
tidy(b_gompertz) %>% 
  kable() %>% 
  kable_classic()

```

```{r nlsglance,echo=TRUE,results=TRUE}
glance(b_gompertz) %>% 
  kable() %>% 
  kable_classic()


```
</div>
## How well does the model predict?

```{r predict,echo=FALSE}
age_series <- seq(0, 22, by = 0.1)

# Make predictions using the model over those times: 
pred <- predict(b_gompertz, list(animal_age = age_series))

# Bind the predictions and age sequence together into a data frame: 
bison_f_predicted <- data.frame(age_series, pred)
```


```{r, fig.align='center'}
# Plot the observed data and predictions together:
ggplot() +
  geom_point(data = knz_bison_age, 
             aes(x = animal_age, y = animal_weight),
             size = 1,
             alpha = 0.2,color="darkorchid") +
  geom_line(data = bison_f_predicted, 
            aes(x = age_series, y = pred),
            color = "blue",
            size = 1) +
  theme_minimal()+
    xlab('Age')+
  ylab('Weight')+
  ggtitle("NLS Model Prediction of bison females")+
  theme(axis.title = element_text(size=28),axis.text = element_text(size=20))+
  theme(plot.title = element_text(size=24,hjust=0.5))
```

## Model results indicate the lowest possible sum of squared error

```{r residual, echo=TRUE,results=TRUE}
model_aug<-broom::augment(b_gompertz)

sum((model_aug$.resid)^2) # Sum of the squared error

```

<br/>

If we compare different model runs from the trace output we can see this is the smallest sum of squared errors. No other model will get lower this number.

## Adding confidence intervals is easy

<div class=small-code>
```{r, echo=TRUE,results=TRUE}
conf<-as_tibble(predFit(b_gompertz,
            newdata = list(animal_age=age_series),
            interval="confidence"),
            level=0.95) 
conf$age=bison_f_predicted$age_series
head(conf,n=4) %>% 
  kable() %>% 
  kable_classic()

#plot+geom_ribbon(data=conf...)
```

Model fits so well, the confidence intervals don't even show on plot.

</div>

```{r confint, eval=FALSE}
ggplot() +
  geom_point(data = knz_bison_age, 
             aes(x = animal_age, y = animal_weight),
             size = 1,
             alpha = 0.2,color="darkorchid") +
  geom_line(data = bison_f_predicted, 
            aes(x = age_series, y = pred),
            color = "blue",
            size = 1) +
  geom_ribbon(data=conf,aes(x=age_series,ymin=lwr,ymax=upr),alpha=0.5,fill="blue")+
  theme_minimal()+
    xlab('Age')+
  ylab('Weight')+
  ggtitle("NLS Model Prediction of bison females")+
  theme(axis.title = element_text(size=28),axis.text = element_text(size=20))+
  theme(plot.title = element_text(size=30,hjust=0.5))
```

